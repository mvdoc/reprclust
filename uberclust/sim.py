#emacs: -*- mode: python-mode; py-indent-offset: 4; tab-width: 4; indent-tabs-mode: nil -*- 
#ex: set sts=4 ts=4 sw=4 noet:
"""Few functions to simulate data for testing various clustering approaches

 COPYRIGHT: Yaroslav Halchenko 2015

 LICENSE: MIT

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to deal
  in the Software without restriction, including without limitation the rights
  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
  copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
  THE SOFTWARE.
"""

__author__ = 'Yaroslav Halchenko'
__copyright__ = 'Copyright (c) 2015 Yaroslav Halchenko'
__license__ = 'MIT'

import numpy as np
import skimage.filter

from mvpa2.base.dataset import vstack as dsvstack
from mvpa2.mappers.fx import mean_group_sample
from mvpa2.datasets.base import Dataset
from mvpa2.mappers.flatten import FlattenMapper
from mvpa2.misc.neighborhood import Sphere

import pylab as pl
# Target sample dissimilarities to "simulate"
DISSIM6 = []

def vector_form(m):
    m = np.asanyarray(m)
    if m.ndim > 1:
        return m[np.triu_indices(m.shape[0], 1)]
    return m

# TODO: while performing filtering, we are probably changing amount of
# variance, so we might want to "re-standardize" whenever smoothing noise

def filter_each_2d(d, sigma):
    if d.ndim == 3:
        # rollaxis is to move "vstacked" last dimension back to its place
        return np.rollaxis(
                 np.array([
                       skimage.filter.gaussian_filter(d[..., i], sigma)
                       for i in xrange(d.shape[2])]),
                 0, 3)
    elif d.ndim == 2:
        return skimage.filter.gaussian_filter(d, sigma)
    else:
        raise ValueError("Din't yet bother for ndim > 3")

def simple_sim1(shape, dissims,
                rois_arrangement='circle',
                roi_neighborhood=Sphere(5),
                nruns=1, nsubjects=1,
                # noise components
                # "Intrinsic signal" -- might be fun for someone to cluster, but irrelevant for us
                intrinsic_n=1, intrinsic_std=0.4, intrinsic_smooth_sigma=2,
                # "Instrumental noise" -- we just add normal for now also with spatial smoothing
                normal_std=0.4, normal_smooth_sigma=2,
                ):
    """Simulate "data" containing similarity matrices with 2 noise components for multiple subjects

    Noise components are:

    - intrinsic noise which is composed from a set of random fields,
    generated by random normal noise with subsequent spatial filtering,
    which are then mixed into each run data with random weights.  They
    are to simulate subject-specific intrinsic signals such as artifacts
    due to motion, possible subject-specific physiological processes

    - random normal noise, also spatially smoothed (should have smaller
    sigma for smoothing probably than for intrinsic noise)

    - TODO: add common across subjects intrinsic noise (e.g. all of them
    have similar blood distribution networks and other physiological
    parameters, and some intrinsic networks, which although similar in
    space would have different mix-in coefficients across subject/runs
    )
    """
    ndissims = len(dissims)

    # first we fisher transform so we can add normal noise
    # check first that we don't have extreme values that might give infinity
    dissims = np.array(dissims)
    dissims = 1. - dissims
    dissims[dissims==1] = 0.99
    dissims[dissims==-1] = -0.99
    # fisher
    dissims = np.arctanh(dissims)

    # generate target clean "picture"
    d = np.asanyarray(dissims[0])
    signal_clean = np.zeros(shape + (len(vector_form(d)),))

    # generate ground truth for clustering
    cluster_truth = np.zeros(shape, dtype='int')

    if rois_arrangement == 'circle':
        radius = min(shape[:2])/4.
        center = np.array((radius*2,) * len(shape)).astype(int)
        # arrange at quarter distance from center
        for i, dissim in enumerate(dissims):
            dissim = vector_form(dissim)
            # that is kinda boring -- the same dissimilarity to each
            # voxel???
            #
            # TODO: come up with a better arrangement/idea, e.g. to
            # generate an MVPA pattern which would satisfy the
            # dissimilarity (not exactly but at least close).  That
            # would make more sense
            roi_center = center.copy()
            roi_center[0] += int(radius * np.cos(2*np.pi*i/ndissims))
            roi_center[1] += int(radius * np.sin(2*np.pi*i/ndissims))
            for coords in roi_neighborhood(roi_center):
                acoords = np.asanyarray(coords)
                if np.all(acoords >= [0]*len(coords)) and \
                   np.all(acoords < signal_clean.shape[:len(coords)]):
                    signal_clean.__setitem__(coords, dissim)
                    cluster_truth.__setitem__(coords, i+1)
    else:
        raise ValueError("I know only circle")

    # Now lets generate per subject and per run data by adding some noise(s)
    # all_signals = []
    dss = []
    for isubject in xrange(nsubjects):
        # Interesting noise, simulating some underlying process which has nothing
        # to do with original design/similarity but having spatial structure which
        # repeats through runs with random weights (consider it to be a principal component)

        # pattern is generated randomly for each subject separately
        # TODO: we need common intrinsic signal as well!!!
        intrinsic_noises = [
            filter_each_2d(np.random.normal(size=signal_clean.shape)*intrinsic_std,
                           intrinsic_smooth_sigma)
            for i in xrange(intrinsic_n)]

        # subject_signals = []
        dss_subject = []
        for run in range(nruns):
            signal_run = signal_clean.copy()
            for intrinsic_noise in intrinsic_noises:
                signal_run += intrinsic_noise * np.random.normal()
            signal_run += filter_each_2d(
                np.random.normal(size=signal_clean.shape)*normal_std,
                normal_smooth_sigma)
            # go back to correlations with inverse of fisher
            signal_run = 1. - np.tanh(signal_run)
            # rollaxis to bring similarities into leading dimension
            ds = Dataset(np.rollaxis(signal_run, 2, 0))
            ds.sa['chunks'] = [run]
            ds.sa['dissimilarity'] = np.arange(len(dissim)) # Lame one for now
            ds_flat = ds.get_mapped(FlattenMapper(shape=ds.shape[1:], space='pixel_indices'))
            dss_subject.append(ds_flat)
            #subject_signals.append(signal_run)
        #all_signals.append(subject_signals)
        ds = dsvstack(dss_subject)
        ds.a['mapper'] = dss_subject[0].a.mapper   # .a are not transferred by vstack
        dss.append(ds)

    # Instrumental noise -- the most banal
    assert(len(dss) == nsubjects)
    assert(len(dss) == nsubjects)
    assert(len(dss[0]) == nruns*len(dissim))

    return signal_clean, cluster_truth, dss

if __name__ == '__main__':
    a_clean, dss = simple_sim1((64, 64), [[1], [0.8], [0.5], [0.3]],
                                 roi_neighborhood=Sphere(6),
                                 nruns=3, nsubjects=2,
                                 intrinsic_n=1, intrinsic_smooth_sigma=5, intrinsic_std=5,
                                 normal_smooth_sigma=1.5, normal_std=4)

    # just a little helper
    def get2d(ds):
        return dss[0].a.mapper.reverse(ds)

    import pylab as pl
    pl.clf()
    DS = dsvstack(dss)
    # Sample plots
    for s in [0, 1]:
        ds2 = get2d(dss[0])
        for r in [0, 1]:
            pl.subplot(3,3,1+r+s*3); pl.imshow(ds2[ds2.sa.chunks == r].samples[0], interpolation='nearest'); pl.ylabel('subj%d' % s);  pl.xlabel('run1');
        pl.subplot(3,3,3+s*3); pl.imshow(get2d(mean_group_sample(['dissimilarity'])(dss[0]).samples)[0], interpolation='nearest'); pl.xlabel('mean');

    ds = dsvstack(dss)
    ds.a['mapper'] = dss[0].a.mapper
    ds_mean = mean_group_sample(['dissimilarity', 'chunks'])(ds)
    for r in [0, 1]:
        ds_mean_run0 = ds.a.mapper.reverse(ds_mean[ds_mean.chunks == r])
        pl.subplot(3,3,1+r+2*3); pl.imshow(ds_mean_run0.samples[0], interpolation='nearest'); pl.ylabel('mean(subj)');  pl.xlabel('run%d' % r)
    ds_global_mean = mean_group_sample(['dissimilarity'])(ds)
    pl.subplot(3,3,3+2*3); pl.imshow(get2d(ds_global_mean).samples[0], interpolation='nearest'); pl.xlabel('mean');

pl.show()
